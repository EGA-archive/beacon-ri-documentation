Documentation Tutorials Create your Beacon
10 steps to create your first beacon
Introduction
This guide will walk you through setting up a GA4GH Beacon v2 instance using the Production Implementation API. You’ll deploy your own Beacon, load example data in Beacon Friendly Format (BFF), and explore how to query it through the API.

As defined by the Global Alliance for Genomics and Health (GA4GH), a Beacon v2 is an open standard for the discovery of genomic (and phenotypic and clinical) data in biomedical research and clinical applications. Its main goal is to facilitate the discovery - and potential retrieval - of genomic variants and biomedical data from different organisational and geographic locations.

If you are here it means you are interested in creating your own beacon!


pin
This will be the steps you'll need to follow:

Installing Beacon 2 Production Implementation API
Converting VCF to Genomic Variations Model
Filling in the CSV Templates
Configuring conf.py
Converting CSV/VCF to BFF
Deploying your beacon instance
1. Installing Beacon 2 Production Implementation API and beacon2-ri-tools-v2
Before getting started, make sure you have the following:

Docker
Docker Compose
git
This tutorial uses Beacon Friendly Format (BFF) files – standardized JSON documents that describe the beacon models. To create the BFF in the easiest way possible (directly from CSV, VCF or phenopacket) we’ll use the Beacon2 ri tools v2, included in the beacon2-pi-api repository. 

First, let’s clone the beacon production implementation repository in your system:

git clone https://github.com/EGA-archive/beacon2-pi-api.git


Ensure port 21017 and 5050 are free for MongoDb and Beacon.

And now let’s build all the necessary containers: 

docker compose up -d --build 

Let’s check if the building was correct. Run the following command:

docker ps

This command will show all the running containers in your computer. You should have built: 

Image	Names
ghcr.io/ega-archive/beacon2-ri-tools-v2:latest

ri-tools

ghcr.io/ega-archive/phenopackets-to-bff:latest

phenopackets-to-BFF

mongo-express

beacon2-pi-api-mongo-express-1

ghcr.io/ega-archive/beacon2-ri-postgres-v2:latest

idp-db

beacon2-pi-api-beaconprod

beaconprod

2. Understanding Beacon Data Models
There are several ways of populating a beacon. To introduce information into a beacon you need to populate one, some, or all of the beacon model entities.

Let’s recapitulate about what model entities the beacon allows: 

Collections (Datasets and Cohorts): groupings of variants or individuals that share something in common: e.g., who belong to the same repository (datasets) or study populations (cohorts).
Genomic variations: unique genomic alterations, e.g., position in a genome, sequence alterations, type, etc. This model can be filled in directly with a VCF or creating a CSV.
Individuals: either patients or healthy controls whose details (including phenotypic and clinical) are stored in the repository.
Biosamples: samples taken from individuals, including details of procedures, dates and times.
Analyses & Runs: details on (a) procedures used for sequencing a biosample (runs), and (b) bioinformatic procedures to identify variants (analyses).

Note that none of these models are mandatory for a beacon to be functional, but the dataset model is mandatory for the API to be queryable. All the beacons will respond with boolean, count and full response. But the response type, hit, miss, all and none  will be available if the dataset model was included in the beacon. This is what these responses mean:
Hit: it returns the positive results of the query per dataset. 
Miss: it returns the datasets where the query did not have a hit. 
All: it returns all the datasets in the beacon. 
None: aggregated results (not splitted by dataset)
If you are interested in using them, make sure to include the datasets model into your beacon instance.


3. Converting VCF to Genomic Variations Model beacon2-ri-tools-v2
The most simple way of converting variant information into the genomicVariations model is directly reading a VCF into Beacon Friendly Format (BFF). To convert directly from a VCF,  all the files into the files_to_read folder and follow the instructions below to correctly fill in the conf.py.


Bear in mind that if your variant information is not in a VCF format you can also fill in the Genomic Variations CSV.

4. Filling in the CSV Templates
Now, let’s see how to fill in the CSV that will be converted by the Beacon RI Tools into the models.

Here you’ll find the CSV templates for all the models.

To correctly fill in all the properties you have two sources of information:

The JSON references schemas which will be really useful if you are used to working with JSON schemas.
The GA4GH Beacon v2 Documentation:
Cohorts
Datasets
Biosamples
genomicVariations
Individuals
Runs
Analyses
Beacon v2 aims to be a flexible tool, thus, again not all the fields are mandatory to be filled in. These are the fields that must be included in each model if you decide to include it:

Model → Required fields
Analyses → id, analysisDate, pipelineName
Biosamples → id, biosampleStatus, sampleOriginType
Cohorts → id, name, cohortType
Datasets → id, name
Individuals → id, sex
Runs → id, biosampleId, runDate
genomic Variations → variantInternalId, variation
5. CSV Formatting Tips and Rules
Here are some things to consider during your CSV creation:

Only “correctly spelled” fields (aka columns) will be inserted into the beacon. You can delete unused columns of your CSV, but don’t modify the headers of the used ones.
Every new row will be appended to the final output as a new and independent document. e.g. If in dataset.csv you have two rows, you will be creating two independent datasets.
To add multiple information in the same row separate it by a pipe '|'. For instance, in genomicVariations, if several samples have the same variant you will need to fill in caseLevelData properties as below. Note that you can leave the pipe empty or write the condition multiple times. e.g:

caseLevelData | biosampleId      caseLevelData | phenotypicEffects | conditionId
SAMPLE2 | SAMPLE3 | SAMPLE4      Alzheimer | |

In this case, this would mean that the variant in that row was found in sample 2, 3, and 4 and it’s associated to Alzheimer.
The info field for each collection is very generic and can be filled with different data. You will need to fill the column data directly with JSON-type data. e.g:

{"info": "This dataset contains synthetic metadata"}

The id field in Biosamples must match the sample IDs in the genomicVariations/VCF model. If these IDs don’t match, the mapping with the variants and the biosamples won’t be correctly performed.
The id in Individuals can match or not the ID in genomicVariations/VCF. If it doesn’t match, for the mapping between individuals and variants to be correctly performed, the field individualsId must be filled in the Biosamples model, mapping the IDs in Individuals and the IDs in genomicVariations/VCF.
The datasetId in Datasets must match the datasetId field in the conf.py (this requirement applies only in the Beacon Production Implementation environment).
6. Configuring conf.py
Now that you’ve already gathered all the necessary information, let’s convert it into Beacon Friendly Format (BFF) using the beacon r.i tools.

First, let’s edit the ./beacon2-pi-api/ri-tools/conf/conf.py:

#### Input and Output files config parameters ####

csv_folder = './csv/examples/'
output_docs_folder='./output_docs/'

#### VCF Conversion config parameters ####

allele_counts=False
reference_genome='GRCh37' # Choose one between NCBI36, GRCh37, GRCh38
datasetId='COVID_pop11_fin_2'
case_level_data=False
exact_heterozygosity=False
num_rows=1500000
verbosity=False

How to fill in config.py?
The csv_folder variable sets the path of a folder containing all the CSVs that will be converted to BFF. Please, don’t modify the path ./csv/.
The output_docs_folder variable sets the folder where your final .json files (BFF format) will be saved once execution of beacon tools finishes. This folder should always be located within the folder 'output_docs', but subdirectories can be created inside it. e.g output_docs_folder='./output_docs/test1'
If you’ve chosen to read variants directly from a VCF instead of filling in the genomicVariations CSV you’ll need to fill in the VCF Conversion config parameters:

The reference_genome is the reference genome the tool will use to map the position of the chromosomes.
Make sure to select the same version as the one used to generate your data.
The datasetId needs to match the id of your datasets.csv or datasets.json file.
This will add a datasetId field in every record to match the record with the dataset it belongs to.
The case_level_data is a boolean parameter ( True or False) which will relate your variants to the samples they belong to.
As we don’t have the biosamples schema, this mapping won’t work. Let’s set it up as False.
The exact_heterozygosity is a boolean parameter ( True or False) that, in case case_level_data is True, then it will classify the biosamples as being heterozygous for either the reference or the alternate allele.
In our case, as case_level_data is False, let’s set this parameter as False too.
The num_rows are the approximate calculation you expect for the total of variants in each VCF there are.
Make sure this is greater than the total variants expected.
The verbosity will give streaming logs with the reason why a variant has been skipped to be inserted.
Recommendation is to leave this as False.
7. Converting CSV/VCF to BFF
Time for the conversion! 
Before preceding ensure that: 
All the CSVs that you’ve created have been saved in the csv folder stated in the conf.py, e.g ./beacon2-pi-api/ri-tools/conf/conf.py/csv/your/path. 
All the VCFs to be converted are saved in./beacon2-pi-api/ri-tools/files/vcf/files_to_read

If this is done, then run:

docker exec -it ri-tools python convert_csvTObff.py

The Beacon Friendly Format JSONs from CSVs will be generated in the output_docs folder, with the name of the collection followed by .json extension, e.g. biosamples.json.
If you have VCFs to convert, then run too:

docker exec -it ri-tools python genomicVariations_vcf.py

And export it from the mongoDB to a JSON BFF with: 

docker exec ri-tools mongoexport --jsonArray --uri "mongodb://
root:example@127.0.0.1:27017/beacon?authSource=admin" --collection
genomicVariations > ./beacon2-pi-api/ri-tools/output_docs//
genomicVariations.json

All these BFF JSONs will be used to populate a mongoDB for beacon usage.
Before moving to the actual installation and population of the beacon, ensure all the following steps have been accomplished:
If reading from VCFs, move the necessary files to files_to_read folder
Fill in CSV templates
Set the correct paths in conf.py
Run: docker exec -it ri-tools python convert_csvTObff.py
If reading from a VCF, run: docker exec -it ri-tools python genomicVariations_vcf.py
Find your BFFs in the output_docs

8. Deploying a Beacon 2 Production Implementation API
To deploy your first beacon we’ll use the automated deployment that relies on the beacon2-pi-api repository that we cloned at the beginning of this tutorial.
By executing the automated deployment we’ll be performing two key actions:
The deployment of your beacon instance
The initial insertion of data

By default, the data saved in the directory ./beacon2-pi-api/beacon/connections/mongo/data/test will be inserted.
Want to insert your own data?
Move your BFFs file to the /data/test directory.
Update the path in the /beacon/connections/mongo/Makefile document.

From the root of the repository, ./beacon2-pi-api, run:

bash mongostart.sh

If the operation is successful, you will have a beacon up and running at http://localhost:5050/api.

This is what you should be seeing:

docker exec -it ri-tools python genomicVariations_vcf.py

Relationship-elements
When searching http://localhost:5050/api we land at the /info entrypoint of the API. To make your own landing page please visit Editing your beacon information webpage and modify the required files before deploying.

9. Reviewing Inserted Entries
Now, let’s move through your beacon and see the data you have inserted. If you did not modify the content of the data folder the test BFFs will have been included.

To visit the inserted data you need to move through the different beacon endpoints:

http://localhost:5050/api/datasets
http://localhost:5050/api/analyses
http://localhost:5050/api/runs
http://localhost:5050/api/cohorts
http://localhost:5050/api/biosamples
http://localhost:5050/api/individuals
http://localhost:5050/api/g_variants
10. Loading more data
From the cloned repository move to:

cd ./beacon2-pi-api/beacon/connections/mongo/data

And copy all the BFFs that you want to insert.
Then, run:

docker exec mongoprod mongoimport --jsonArray --uri "mongodb://root:example@127.0.0.1:27017/beacon?authSource=admin" --file /data/datasets.json --collection datasets
docker exec mongoprod mongoimport --jsonArray --uri "mongodb://root:example@127.0.0.1:27017/beacon?authSource=admin" --file /data/individuals.json --collection individuals
docker exec mongoprod mongoimport --jsonArray --uri "mongodb://root:example@127.0.0.1:27017/beacon?authSource=admin" --file /data/cohorts.json --collection cohorts
docker exec mongoprod mongoimport --jsonArray --uri "mongodb://root:example@127.0.0.1:27017/beacon?authSource=admin" --file /data/analyses.json --collection analyses
docker exec mongoprod mongoimport --jsonArray --uri "mongodb://root:example@127.0.0.1:27017/beacon?authSource=admin" --file /data/biosamples.json --collection biosamples
docker exec mongoprod mongoimport --jsonArray --uri "mongodb://root:example@127.0.0.1:27017/beacon?authSource=admin" --file /data/genomicVariations.json --collection genomicVariations
docker exec mongoprod mongoimport --jsonArray --uri "mongodb://root:example@127.0.0.1:27017/beacon?authSource=admin" --file /data/runs.json --collection runs
docker exec mongoprod mongoimport --jsonArray --uri "mongodb://root:example@127.0.0.1:27017/beacon?authSource=admin" --file /data/targets.json --collection targets
docker exec mongoprod mongoimport --jsonArray --uri "mongodb://root:example@127.0.0.1:27017/beacon?authSource=admin" --file /data/caseLevelData.json --collection caseLevelData

Note that you will only need to run the command for the collections that you have new data. 

Each time data is imported into the beacon, indexes need to be created for the queries to run smoothly. Do it by running:

docker exec beaconprod python -m beacon.connections.mongo.reindex
